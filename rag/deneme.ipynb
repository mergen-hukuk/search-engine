{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/19 19:42:26 WARN Utils: Your hostname, Anls-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 161.9.118.206 instead (on interface en0)\n",
      "24/12/19 19:42:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/19 19:42:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TextVectorization\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"text.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, lower, split\n",
    "\n",
    "df_clean = df.withColumn(\"clean_text\", lower(regexp_replace(\"t1\", \"[^a-zA-Z\\\\s]\", \"\")))\n",
    "df_tokens = df_clean.withColumn(\"tokens\", split(\"clean_text\", \"\\\\s+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/19 19:42:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: t1, \n",
      " Schema: t1, _c1\n",
      "Expected: _c1 but found: \n",
      "CSV file: file:///Users/anildervis/Documents/ITU/YZV411/Project/text.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-----------+--------------------+\n",
      "|       t1| _c1|clean_text|     tokens|        raw_features|\n",
      "+---------+----+----------+-----------+--------------------+\n",
      "|jaskldbsa|NULL| jaskldbsa|[jaskldbsa]|(10000,[6118],[1.0])|\n",
      "| jaskdnsa|NULL|  jaskdnsa| [jaskdnsa]|(10000,[3561],[1.0])|\n",
      "| ashjdlsa|NULL|  ashjdlsa| [ashjdlsa]|(10000,[5049],[1.0])|\n",
      "+---------+----+----------+-----------+--------------------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|features                           |\n",
      "+-----------------------------------+\n",
      "|(10000,[6118],[0.6931471805599453])|\n",
      "|(10000,[3561],[0.6931471805599453])|\n",
      "|(10000,[5049],[0.6931471805599453])|\n",
      "+-----------------------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\", numFeatures=10000)\n",
    "featurized_data = hashing_tf.transform(df_tokens)\n",
    "\n",
    "print(featurized_data.show())\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized_data)\n",
    "rescaled_data = idf_model.transform(featurized_data)\n",
    "\n",
    "print(rescaled_data.select(\"features\").show(truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"vector\")\n",
    "dataset = assembler.transform(rescaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def compute_similarity(vector1, vector2):\n",
    "    return cosine_similarity([vector1], [vector2])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "user_vector = Vectors.dense(np.random.rand(10000))\n",
    "user_vector_broadcast = spark.sparkContext.broadcast(user_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "similarity_udf = udf(lambda x: float(compute_similarity(user_vector_broadcast.value, x)), DoubleType())\n",
    "df_similarities = dataset.withColumn(\"similarity\", similarity_udf(\"vector\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/19 19:42:38 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: t1, \n",
      " Schema: t1, _c1\n",
      "Expected: _c1 but found: \n",
      "CSV file: file:///Users/anildervis/Documents/ITU/YZV411/Project/text.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|       t1| _c1|clean_text|     tokens|        raw_features|            features|              vector|          similarity|\n",
      "+---------+----+----------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|jaskldbsa|NULL| jaskldbsa|[jaskldbsa]|(10000,[6118],[1.0])|(10000,[6118],[0....|(10000,[6118],[0....|0.010676062180195093|\n",
      "| jaskdnsa|NULL|  jaskdnsa| [jaskdnsa]|(10000,[3561],[1.0])|(10000,[3561],[0....|(10000,[3561],[0....|0.014932668346377753|\n",
      "| ashjdlsa|NULL|  ashjdlsa| [ashjdlsa]|(10000,[5049],[1.0])|(10000,[5049],[0....|(10000,[5049],[0....|0.012092973083839812|\n",
      "+---------+----+----------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(df_similarities.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[t1: string, _c1: string, clean_text: string, tokens: array<string>, raw_features: vector, features: vector, vector: vector, similarity: double]\n",
      "+--------------------+--------------------+\n",
      "|              vector|          similarity|\n",
      "+--------------------+--------------------+\n",
      "|(10000,[3561],[0....|0.014932668346377753|\n",
      "|(10000,[5049],[0....|0.012092973083839812|\n",
      "|(10000,[6118],[0....|0.010676062180195093|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_matches = df_similarities.orderBy(\"similarity\", ascending=False).limit(10)\n",
    "print(top_matches)\n",
    "top_matches.select(\"vector\", \"similarity\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
